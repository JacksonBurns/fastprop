# example_fastprop_train_config.yml
#
# This file shows and describes the configurable options for fastprop, but is not itself
# a runnable file. See the `benchmarks` directory for runnable example files.


# Generic Metadata
#
# Directory for output files (cached descriptors, logs, etc.)
output_directory: /path/to/output/dir
# Random seed to control torch as well as data sampling
random_seed: 42
# Type of problem (can be regression or one of these classification types: binary, multiclass, multilabel)
problem_type: regression
# Source of data
input_file: /path/to/data.csv
# Header for column containing targets values (specify multiple with a space in between)
target_columns: lipohilicity permittivity
# Header for column containing SMILES strings
smiles_column: smiles
# run hyperparameter optimization
optimize: False # True
# This will overwrite hidden_size and fnn_layers!

# Featurization
#
# Which set of descriptors to calculate (either all or optimized)
descriptors: all
# Allow caching of descriptors
enable_cache: True
#
# or
#
# Provide your own descriptors or those calculated by mordredcommunity from its CLI
precomputed: /path/to/precomputed/descriptors.csv

# Preprocessing
#
# Rescale descriptors from their initial range to 0->1
rescaling: True
# Drop descriptors which are invariant across the entire dataset
zero_variance_drop: False
# Drop descriptors which are colinear
colinear_drop: False

# Training
#
# Number of layers in the FNN
fnn_layers: 2
# Height of each layer in the FNN
hidden_size: 1800
# Initial learning rate for Adam optimizer
learning_rate: 0.0001
# Batch size
batch_size: 2048
# Number of epochs
number_epochs: 1000
# Size of the training, validation, and testing sets
train_size: 0.8
val_size: 0.05
test_size: 0.15
# Checkpoint from a previous run from which to restart (optional)
checkpoint: /path/to/checkpoint.ckpt
# Algorithm for data sampling, i.e. random, see all available samplers
# here: https://github.com/JacksonBurns/astartes?tab=readme-ov-file#implemented-sampling-algorithms
sampler: random
# number of epochs to wait before stopping early - avoid overfitting
patience: 5
